apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: {{deployment_name}}
  labels:
    # must have this label
    kalavai.lws.name: {{deployment_name}}
spec:
  replicas: {{replicas}}
  leaderWorkerTemplate:
    size: {{num_workers}}
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          role: leader
          kalavai.lws.name: {{deployment_name}}
      spec:
        runtimeClassName: nvidia
        containers:
        - name: aphrodite-leader
          image: docker.io/bundenth/ray-aphrodite:v1.0.11
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          - name: HF_HOME
            value: /home/ray/cache
          - name: TMPDIR
            value: /home/ray/cache/tmp
          command:
            - sh
            - -c
            - "/home/ray/workspace/ray_init.sh leader --ray_cluster_size={{num_workers}} --ray_object_store_memory={{shmem_size}};
                sleep 30;
                nvidia-smi;
                ray status;
                /home/ray/workspace/run_model.sh \
                  --repo_id={{repo_id}} \
                  --model_filename=$model_filename \
                  --extra='{{extra}}' \
                  --tensor_parallel_size={{tensor_parallel_size}} \
                  --pipeline_parallel_size={{pipeline_parallel_size}} \
                  --local_dir=/home/ray/cache;
                sleep 30"
          resources:
            requests:
              cpu: "{{cpus}}"
              memory: {{memory}}Gi
              nvidia.com/gpu: "{{gpus}}"
              nvidia.com/gpucores: 100
            limits:
              cpu: "{{cpus}}"
              memory: {{memory}}Gi
              nvidia.com/gpu: "{{gpus}}"
              nvidia.com/gpucores: 100
          ports:
          # if use 8080 as exposed port (if required)
          - containerPort: 8080
          readinessProbe:
            tcpSocket:
              port: 8080
            initialDelaySeconds: 90
            periodSeconds: 30
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm   
            - name: cache
              mountPath: /home/ray/cache
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{shmem_size}}
        - name: cache
          persistentVolumeClaim:
            claimName: {{storage}}
    workerTemplate:
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
      spec:
        runtimeClassName: nvidia
        containers:
        - name: aphrodite-worker
          image: docker.io/bundenth/ray-aphrodite:v1.0.11
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          - name: HF_HOME
            value: /home/ray/cache
          - name: TMPDIR
            value: /home/ray/cache/tmp
          command:
            - sh
            - -c
            - "nvidia-smi;
                /home/ray/workspace/ray_init.sh worker --ray_address=$LWS_LEADER_ADDRESS --ray_object_store_memory={{shmem_size}} --ray_block=1"
          resources:
            requests:
              cpu: "{{cpus}}"
              memory: {{memory}}Gi
              nvidia.com/gpu: "{{gpus}}"
              nvidia.com/gpucores: 100
            limits:
              cpu: "{{cpus}}"
              memory: {{memory}}Gi
              nvidia.com/gpu: "{{gpus}}"
              nvidia.com/gpucores: 100
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{shmem_size}}