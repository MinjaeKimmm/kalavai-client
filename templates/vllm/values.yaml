- name: deployment_name
  value: vllm-deployment-1
  default: vllm-deployment-1
  description: "Name of the deployment job"

- name: storage
  value: "pool-cache"
  default: "pool-cache"
  description: "Pool storage to use to cache model weights"

- name: remote_workers
  value: "0"
  default: "0"
  description: "Number of remote workers (for tensor and pipeline parallelism). This is in addition to the main node"

- name: model_id
  value: null
  default: null
  description: "Huggingface model id to load"

- name: hf_token
  value: <yout token>
  default: null
  description: "Huggingface token, required to load model weights"

- name: cpus
  value: "4"
  default: "4"
  description: "CPUs per single worker (final one = cpus * num_workers)"

- name: gpus
  value: "1"
  default: "1"
  description: "GPUs per single worker (final one = gpus * num_workers)"

- name: memory
  value: "4"
  default: "4"
  description: "RAM memory per single worker (final one = memory * num_workers)"

- name: tensor_parallel_size
  value: "1"
  default: "1"
  description: "Tensor parallelism (use the number of GPUs per node)"

- name: pipeline_parallel_size
  value: "1"
  default: "1"
  description: "Pipeline parallelism (use the number of nodes)"

- name: shmem_size
  value: "4000000000"
  default: "4000000000"
  description: "Size of the shared memory volume (in bytes; 1GB is 1000000000)"

- name: extra
  value: "--dtype float16 --enforce-eager --swap-space 0"
  default: --dtype float16 --enforce-eager --swap-space 0
  description: "Extra parameters to pass to the vLLM server. See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server"

# must contain the list of ports to enable in the leader node
- name: endpoint_ports
  value: "8080"
  default: "8080"
  description: "[DO NOT CHANGE] Ports opened on master node (will be shown as endpoints for the job)"