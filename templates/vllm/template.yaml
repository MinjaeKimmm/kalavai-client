apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_name}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_name}}
spec:
  queue: default
  schedulerName: volcano
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  policies: 
  - event: PodEvicted # Restart the job when a pod is evicted.
    action: RestartJob
  tasks:
  - replicas: 1   # One ps pod specified
    name: server
    template: # Definition of the ps pod
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          role: leader
          kalavai.job.name: {{deployment_name}}
      spec:
        runtimeClassName: nvidia
        containers:
        - name: vllm-leader
          image: docker.io/bundenth/ray-vllm:latest #v1.1.4
          command:
          - sh
          - -c
          - |
            RAY_BACKEND_LOG_LEVEL=error /home/ray/workspace/ray_init.sh leader --ray_cluster_size=$(({{remote_workers}}+1)) --ray_object_store_memory={{shmem_size}};
              sleep 30;
              nvidia-smi;
              ray status;
              /home/ray/workspace/run_model.sh \
                --model_id={{model_id}} \
                --extra='{{extra}}' \
                --tensor_parallel_size={{tensor_parallel_size}} \
                --pipeline_parallel_size={{pipeline_parallel_size}} \
                --cache_dir=/home/ray/cache \
                --lora_modules="{{lora_modules}}";
              sleep 30
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          - name: HF_HOME
            value: /home/ray/cache
          - name: TMPDIR
            value: /home/ray/cache/tmp
          ports:
          - containerPort: 8080
            name: model-port
          - containerPort: 8265
            name: dashboard-port
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm   
            - name: cache
              mountPath: /home/ray/cache
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{shmem_size}}
        - name: cache
          persistentVolumeClaim:
            claimName: {{storage}}
        restartPolicy: Never
  - replicas: {{remote_workers}}
    name: worker
    policies:
    - event: TaskCompleted  # The job will be marked as completed when two worker pods finish tasks.
      action: CompleteJob
    template: # Definition of worker pods
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          kalavai.job.name: {{deployment_name}}
      spec:
        runtimeClassName: nvidia
        containers:
        - name: vllm-worker
          image: docker.io/bundenth/ray-vllm:latest #v1.1.4
          command:
          - sh
          - -c
          - |
            PS_HOST=`head /etc/volcano/server.host`;
            nvidia-smi;
            RAY_BACKEND_LOG_LEVEL=error /home/ray/workspace/ray_init.sh worker --ray_address=$PS_HOST --ray_port=6379 --ray_object_store_memory={{shmem_size}} --ray_block=1
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          - name: HF_HOME
            value: /home/ray/cache
          - name: TMPDIR
            value: /home/ray/cache/tmp
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{shmem_size}}
        restartPolicy: Never
